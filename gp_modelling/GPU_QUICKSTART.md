# GPU-Accelerated GP Training - Quick Start Guide

## ğŸš€ Quick Start (3 Steps)

### 1. Install Dependencies
```bash
# Install GPU requirements
pip install -r requirements_gpu.txt

# Install PyTorch with CUDA (check your CUDA version with nvidia-smi)
# For CUDA 11.8:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 2. Check GPU Setup
```bash
python check_gpu_setup.py
```

This will:
- âœ… Verify PyTorch and GPyTorch installation
- âœ… Detect GPU and VRAM
- âœ… Calculate optimal chunk sizes
- âœ… Run a test GP training
- âœ… Provide configuration recommendations

### 3. Run Training
```bash
# GPU version (recommended if you have GPU)
python train_gp_models_gpu.py

# Or compare both versions
python compare_cpu_gpu.py
```

## ğŸ“Š What to Expect

### Performance Gains
- **Small datasets (<5k samples)**: 2-3x faster
- **Medium datasets (5-10k samples)**: 5-10x faster
- **Large datasets (>10k samples)**: 10-20x faster

### Memory Usage (16GB VRAM)
The script automatically chunks data to fit:

| Features | Chunk Size | VRAM Usage |
|----------|------------|------------|
| 3        | ~6,000     | ~12 GB     |
| 7        | ~5,500     | ~13 GB     |
| 10       | ~5,000     | ~13 GB     |

## ğŸ”§ Configuration

Edit `train_gp_models_gpu.py` if needed:

```python
def main():
    mill_number = 8
    start_date = "2025-08-01"
    end_date = "2025-10-19"
    
    config = PipelineConfig.create_default(mill_number, start_date, end_date)
    
    # Adjust VRAM limit (default: 14GB from 16GB total)
    trainer = GaussianProcessCascadeTrainerGPU(config, max_vram_gb=14.0)
    trainer.run()
```

## ğŸ“ Output Files

All saved to `gp_modelling/output/mill_gp_gpu_XX/`:

```
mill_gp_gpu_08/
â”œâ”€â”€ process_model_DensityHC.pth          # Process models
â”œâ”€â”€ process_model_PulpHC.pth
â”œâ”€â”€ process_model_PressureHC.pth
â”œâ”€â”€ process_model_CirculativeLoad.pth
â”œâ”€â”€ quality_model.pth                     # Quality model
â”œâ”€â”€ *_scaler.pkl                          # Feature scalers
â”œâ”€â”€ metadata.json                         # Training config & metrics
â”œâ”€â”€ gpu_info.json                         # GPU usage stats
â””â”€â”€ plots/
    â”œâ”€â”€ *_train_predictions.png           # Training plots
    â”œâ”€â”€ *_test_predictions.png            # Test plots
    â”œâ”€â”€ *_uncertainty.png                 # Uncertainty analysis
    â””â”€â”€ cascade_validation.png            # Cascade validation
```

## ğŸ› Troubleshooting

### "CUDA out of memory"
```python
# Reduce VRAM usage
trainer = GaussianProcessCascadeTrainerGPU(config, max_vram_gb=12.0)
```

### "No GPU detected"
```bash
# Check CUDA installation
nvidia-smi

# Check PyTorch CUDA support
python -c "import torch; print(torch.cuda.is_available())"
```

### Slow training despite GPU
- Check GPU utilization: `nvidia-smi -l 1`
- Verify CUDA version matches PyTorch
- Close other GPU applications

## ğŸ“ˆ Monitoring Training

### Watch GPU usage in real-time
```bash
# Option 1: nvidia-smi
nvidia-smi -l 1

# Option 2: watch
watch -n 1 nvidia-smi
```

### Check logs
```bash
# Training progress
tail -f train_gp_models_gpu.log

# Look for:
# - "GPU detected: [GPU Name]"
# - "Optimal chunk size: XXXX samples"
# - "Peak VRAM usage: XX.XX GB"
```

## ğŸ”„ CPU vs GPU Comparison

Run both versions side-by-side:

```bash
python compare_cpu_gpu.py
```

This will:
1. Train models with CPU version
2. Train models with GPU version
3. Compare training times
4. Compare model performance
5. Generate comparison report

## ğŸ’¾ Loading Trained Models

```python
import torch
import pickle
from pathlib import Path

# Load model state
model_path = Path("output/mill_gp_gpu_08/process_model_DensityHC.pth")
checkpoint = torch.load(model_path)

model_state = checkpoint['model_state_dict']
likelihood_state = checkpoint['likelihood_state_dict']

# Load scaler
scaler_path = Path("output/mill_gp_gpu_08/process_model_DensityHC_scaler.pkl")
with open(scaler_path, 'rb') as f:
    scaler = pickle.load(f)

# To use the model, you'll need to reconstruct it
# See train_gp_models_gpu.py for ExactGPModel definition
```

## ğŸ¯ Key Differences from CPU Version

| Aspect | CPU Version | GPU Version |
|--------|-------------|-------------|
| **Library** | scikit-learn | GPyTorch + PyTorch |
| **Speed** | Baseline | 5-20x faster |
| **Memory** | RAM | VRAM (chunked) |
| **Model Format** | `.pkl` | `.pth` |
| **Output Dir** | `models/` | `models_gpu/` |
| **Dependencies** | numpy, sklearn | torch, gpytorch |

## âœ… Verification Checklist

Before running production training:

- [ ] GPU detected by `check_gpu_setup.py`
- [ ] CUDA version matches PyTorch installation
- [ ] Sufficient VRAM available (check with `nvidia-smi`)
- [ ] Test training completed successfully
- [ ] Optimal chunk size calculated
- [ ] No other GPU-intensive applications running

## ğŸ“š Additional Resources

- **Full Documentation**: See `README_GPU.md`
- **GPU Setup Checker**: `python check_gpu_setup.py`
- **Performance Comparison**: `python compare_cpu_gpu.py`
- **GPyTorch Docs**: https://gpytorch.ai/
- **PyTorch CUDA Setup**: https://pytorch.org/get-started/locally/

## ğŸ†˜ Need Help?

1. **Check GPU setup**: `python check_gpu_setup.py`
2. **Review logs**: `train_gp_models_gpu.log`
3. **Monitor GPU**: `nvidia-smi -l 1`
4. **Verify CUDA**: `nvidia-smi` and `torch.cuda.is_available()`

## ğŸ‰ Success Indicators

You'll know it's working when you see:

```
âœ“ GPU detected: NVIDIA GeForce RTX 4080
  VRAM: 16.00 GB
  
Optimal chunk size: 5500 samples
Estimated VRAM per chunk: 12.50 GB

Training GP model: MV â†’ DensityHC
  Training on full dataset (4523 samples)...
    Iteration 20/100 - Loss: 2.3456
    Iteration 40/100 - Loss: 1.8234
    ...
  âœ“ Training complete
  Train: RÂ²=0.9234, RMSE=0.4567, Ïƒ=0.1234
  Test:  RÂ²=0.9123, RMSE=0.4890, Ïƒ=0.1345

Peak VRAM usage: 11.23 GB
```

Happy training! ğŸš€
